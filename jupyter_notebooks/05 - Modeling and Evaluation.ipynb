{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1134373",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49c52e",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "Predict medical insurance charges using customer profile information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd4cbf",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "- Processed customer dataset with feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeb40a3",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "- Trained ML regression model.\n",
    "- Feature importance ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e915f7",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ddf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d8a35",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ba0828",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a39845",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450640f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df6efcc",
   "metadata": {},
   "source": [
    "# Load Engineered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_path = 'outputs/datasets/engineered/insurance_engineered.csv'\n",
    "df = pd.read_csv(df_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e946faf",
   "metadata": {},
   "source": [
    "Stop displaying warnings messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67840326",
   "metadata": {},
   "source": [
    "# ML Pipeline with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ea052",
   "metadata": {},
   "source": [
    "## ML pipeline for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80aabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Feature Engineering\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "\n",
    "def PipelineDataCleaningAndFeatureEngineering():\n",
    "    # Define the pipeline for data cleaning and feature engineering\n",
    "    pipeline_base = Pipeline([\n",
    "        ('onehot_encoding', OneHotEncoder(drop_last=True)),\n",
    "        ('correlated_selection', SmartCorrelatedSelection(\n",
    "            method='pearson',\n",
    "            threshold=0.6,\n",
    "            selection_method='variance'))\n",
    "    ])\n",
    "    \n",
    "    return pipeline_base\n",
    "\n",
    "PipelineDataCleaningAndFeatureEngineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b56975",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14285814",
   "metadata": {},
   "source": [
    "## ML Pipeline for Modelling and Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63632cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Define a pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def PipelineClf(model):\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feat_selection\", SelectFromModel(model)),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db358a50",
   "metadata": {},
   "source": [
    "**Hyperparameter Optimisation**\n",
    "\n",
    "This is the process of tuning the hyperparameters of a machine learning model to improve its performance. It involves searching for the best combination of hyperparameters that yield the highest performance on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class HyperparameterOptimizationSearch(BaseEstimator):\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            model = PipelineClf(self.models[key])\n",
    "\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score',\n",
    "                   'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8501bb",
   "metadata": {},
   "source": [
    "## Split Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cd78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and Target (y)\n",
    "X = df.drop('charges', axis=1)\n",
    "y = df['charges']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}  y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}  y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b93e7e",
   "metadata": {},
   "source": [
    "## Grid Search CV - Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde0392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_quick_search = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_quick_search = {\n",
    "    \"LinearRegression\": {},\n",
    "    \"DecisionTreeRegressor\": {\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "        'model__min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    \"RandomForestRegressor\": {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "        'model__min_samples_leaf': [1, 2],\n",
    "        'model__bootstrap': [True]\n",
    "    },\n",
    "    \"GradientBoostingRegressor\": {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.1, 0.2],\n",
    "        'model__max_depth': [3, 5],\n",
    "        'model__subsample': [0.8, 1.0],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "        'model__min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    \"XGBRegressor\": {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.1, 0.2],\n",
    "        'model__max_depth': [3, 5],\n",
    "        'model__subsample': [0.8, 1.0],\n",
    "        'model__colsample_bytree': [0.8, 1.0]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e076d",
   "metadata": {},
   "source": [
    "**Run Grid Search CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5bdd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Grid Search\n",
    "search = HyperparameterOptimizationSearch(\n",
    "    models=models_quick_search,\n",
    "    params=params_quick_search\n",
    ")\n",
    "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c3148",
   "metadata": {},
   "source": [
    "Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeabbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "print(grid_search_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b302eda",
   "metadata": {},
   "source": [
    "**Evaluate the Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb272ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    print(\"Model Evaluation \\n\")\n",
    "    print(\"* Train Set\")\n",
    "    regression_evaluation(X_train, y_train, pipeline)\n",
    "    print(\"* Test Set\")\n",
    "    regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    prediction = pipeline.predict(X)\n",
    "    print('R2 Score:', r2_score(y, prediction).round(3))\n",
    "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))\n",
    "    print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))\n",
    "    print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "    print(\"\\n\")\n",
    "\n",
    "best_model = grid_search_summary.iloc[0]['estimator']\n",
    "print(\"Best Model:\", best_model)\n",
    "best_regressor_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test, best_regressor_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb427f",
   "metadata": {},
   "source": [
    "**Do an extensive search on the most suitable algorithm to find the best hyperparameter configuration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38684c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=0, objective='reg:squarederror'),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    \"XGBRegressor\": {\n",
    "        'model__n_estimators': [100, 300],\n",
    "        'model__learning_rate': [0.1, 0.05, 0.01],\n",
    "        'model__max_depth': [3, 6, 10],\n",
    "        'model__subsample': [0.8, 1.0],\n",
    "        'model__colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "search = HyperparameterOptimizationSearch(\n",
    "    models=models_search,\n",
    "    params=params_search\n",
    ")\n",
    "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a2792",
   "metadata": {},
   "source": [
    "We can see the best score is achieved by the `XGBRegressor` extensive search is lower then the `RandonForestRegressor`. \n",
    "\n",
    "We will run the extensive search on `RandonForestRegressor` model to compare the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c12276",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    \"RandomForestRegressor\": {\n",
    "        'model__n_estimators': [100, 300],\n",
    "        'model__max_depth': [10, 20, None],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "        'model__min_samples_leaf': [1, 2, 4],\n",
    "        'model__bootstrap': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "search = HyperparameterOptimizationSearch(\n",
    "    models=models_search,\n",
    "    params=params_search\n",
    ")\n",
    "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aec312",
   "metadata": {},
   "source": [
    "**According to the scores results, RandomForestRegressor seem to be the best model for this dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fbebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9eacf0",
   "metadata": {},
   "source": [
    "Parameters for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f18ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search_pipelines[best_model].best_params_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472ef62",
   "metadata": {},
   "source": [
    "The best clf pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
    "pipeline_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a6892",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987c3dd",
   "metadata": {},
   "source": [
    "## Assess feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get selected features from the pipeline\n",
    "selected_features = X_train.columns[pipeline_clf.named_steps['feat_selection'].get_support()]\n",
    "importances = pipeline_clf.named_steps['model'].feature_importances_\n",
    "\n",
    "# Create importance dataframe\n",
    "df_feature_importance = (\n",
    "    pd.DataFrame({'Feature': selected_features, 'Importance': importances})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "# Save ordered list of best features\n",
    "best_features = df_feature_importance['Feature'].tolist()\n",
    "\n",
    "# Print top features\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \")\n",
    "print(df_feature_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_feature_importance, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title(\"Feature Importance - RandomForestRegressor\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"outputs/ml_pipeline/v1/feature_importance_rf.png\", dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e61574a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9fb21",
   "metadata": {},
   "source": [
    "## Evaluate Pipeline on Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    prediction = pipeline.predict(X)\n",
    "    print('R2 Score:', r2_score(y, prediction).round(3))\n",
    "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))\n",
    "    print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))\n",
    "    print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "    print(\"\\n\")\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    print(\"Model Evaluation \\n\")\n",
    "    print(\"* Train Set\")\n",
    "    regression_evaluation(X_train, y_train, pipeline)\n",
    "    print(\"* Test Set\")\n",
    "    regression_evaluation(X_test, y_test, pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, pipeline_clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c3c6ea",
   "metadata": {},
   "source": [
    "We used R2 Score, MAE and RMSE to evaluate the model performance, since Confusion Matrix and Accuracy are not suitable for regression problems.\n",
    "\n",
    "**Evaluation Conclusion**\n",
    "\n",
    "The Random Forest Regressor, demonstrates strong predictive performance and generalization capability in estimating medical insurance costs. Its test set R² score of 0.841 indicates that the model explains approximately 84.1% of the variance in insurance charges for unseen data.\n",
    "The model achieved a Mean Absolute Error (MAE) of approximately 2892, meaning that on average, its predictions deviate from the true insurance charges by about $2,892, which is acceptable given the typical range of medical costs. Additionally, the Root Mean Squared Error (RMSE) of around $4,880 confirms that the model performs well without being heavily skewed by large outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3513f",
   "metadata": {},
   "source": [
    "# Push files to Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29991d5",
   "metadata": {},
   "source": [
    "We will generate the following file\n",
    "* Train set\n",
    "* Test set\n",
    "* Data cleaning and Feature Engineering pipeline\n",
    "* Modeling pipeline\n",
    "* features importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "version = \"v1\"\n",
    "file_path = f\"outputs/ml_pipelines/{version}\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(file_path)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcede993",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa85a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de306084",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce6b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58479825",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2523f5",
   "metadata": {},
   "source": [
    "## ML Pipelines: Feature Engineering and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ed872",
   "metadata": {},
   "source": [
    "Pipeline responsible for Data Cleaning and Feature Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ca937",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
    "pipeline_data_cleaning_feat_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=pipeline_data_cleaning_feat_eng ,\n",
    "            filename=f\"{file_path}/clf_pipeline_data_cleaning_feat_eng.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448b1ea",
   "metadata": {},
   "source": [
    "Pipeline responsible for Feature Scaling, and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0664112",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=pipeline_clf ,\n",
    "            filename=f\"{file_path}/clf_pipeline_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d19ee",
   "metadata": {},
   "source": [
    "## Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b158b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e729c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
