{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Apply numerical transformations\n",
        "* Encode ordinal categorical features\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* output/datasets/cleaned/TrainSetCleaned.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Dataset with encoded features\n",
        "* Dataset insights for Modeling \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "TrainSet = pd.read_csv('outputs/datasets/cleaned/TrainSetCleaned.csv')\n",
        "TrainSet.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vizualize Numerical Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will help us confirm skewness and outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numerical_cols = ['age', 'bmi', 'children', 'charges']\n",
        "\n",
        "# Plot histogram and boxplot\n",
        "def plot_histogram_and_boxplot(df, cols):\n",
        "    for col in cols:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "        \n",
        "        sns.histplot(df[col], kde=True, ax=axes[0])\n",
        "        axes[0].set_title(f'Histogram of {col}')\n",
        "        \n",
        "        sns.boxplot(x=df[col], ax=axes[1])\n",
        "        axes[1].set_title(f'Boxplot of {col}')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Run the visualization\n",
        "plot_histogram_and_boxplot(TrainSet, numerical_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Age and BMI transformation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "TrainSet_copy = TrainSet.copy()\n",
        "\n",
        "# Apply to age and bmi\n",
        "TrainSet_copy[['age', 'bmi']] = scaler.fit_transform(TrainSet_copy[['age', 'bmi']])\n",
        "\n",
        "# Check summary stats\n",
        "TrainSet_copy[['age', 'bmi']].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We applied encoding to the Age and BMI features. We will use the `StandardScaler` from `sklearn.preprocessing` to encode these features. On a demonstration, we will wrap the transformations into a clean pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from feature_engine.wrappers import SklearnTransformerWrapper\n",
        "\n",
        "# Define columns\n",
        "num_features_to_scale = ['age', 'bmi']\n",
        "\n",
        "# Pipeline for numerical columns\n",
        "\n",
        "num_pipeline = Pipeline([('scaler', SklearnTransformerWrapper(transformer=StandardScaler(),\n",
        "                                             variables=num_features_to_scale))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will apply categorical encoding to your features. You have the following categorical variables:\n",
        "- sex (binary)\n",
        "- smoker (binary)\n",
        "- region (nominal â€” no natural order)\n",
        "\n",
        "We will one `OrdinalEncoder` since it can handle both categories, and combine with the numerical transformation in a single pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "# Categorical columns\n",
        "cat_features = ['sex', 'smoker', 'region']\n",
        "\n",
        "# Categorical encoder pipeline\n",
        "cat_pipeline = Pipeline([\n",
        "    ('ordinal_encoder', OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                           variables=cat_features))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Join the numerical and categorical pipelines**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full preprocessor\n",
        "full_pipeline = Pipeline([\n",
        "        ('ordinal_encoder', OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                           variables=cat_features)),\n",
        "        ('scaler', SklearnTransformerWrapper(transformer=StandardScaler(),\n",
        "                                             variables=num_features_to_scale))\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Check if the pipeline works**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"DataSet before preprocessing:\\n{TrainSet.head(10)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see the DataFrame with the all encoded features. We used the `OrdinalEncoder` from `sklearn.preprocessing` to encode categorical features, `StandardScaler` to scale numerical features. The `Pipeline` from `sklearn.pipeline` allows us to chain these transformations together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if the pipeline works\n",
        "preprocessed_data = full_pipeline.fit_transform(TrainSet.drop(columns=['charges']))\n",
        "print(f\"DataSet after processing:\\n{preprocessed_data.head(10)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation Matrix of Encoded Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check correlation matrix of the processed data\n",
        "# Concatenate charges to preprocessed_data\n",
        "preprocessed_with_target = preprocessed_data.copy()\n",
        "preprocessed_with_target['charges'] = TrainSet['charges'].values\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = preprocessed_with_target.corr()\n",
        "\n",
        "print(correlation_matrix)\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix of Preprocessed Data (with Target)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push files to Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/feature_engineered')\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "# Save the dataframe to a CSV file in the outputs folder\n",
        "preprocessed_with_target.to_csv('outputs/datasets/feature_engineered/insurance_fe.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions and Next Steps\n",
        "\n",
        "## Conclusions\n",
        "* We have successfully done the following:\n",
        "  - Applied numerical transformations to the dataset.\n",
        "  - Encoded ordinal categorical features using `OrdinalEncoder`.\n",
        "* Final feature set includes:\n",
        "    - age, bmi (standardized)\n",
        "    - children (int, unchanged)\n",
        "    - sex, smoker, region (ordinal encoded)\n",
        "* Important note: 'charges' is was not part of the feature set, as it is the target variable.\n",
        "\n",
        "# Next Steps\n",
        "* We will now proceed to build the pipelines for modeling and feature engineering based on the insights gained from this feature engineering step."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
